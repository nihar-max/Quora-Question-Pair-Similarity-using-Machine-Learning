{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# exctract word2vec vectors\n",
    "# https://github.com/explosion/spaCy/issues/1721\n",
    "# http://landinghub.visualstudio.com/visual-cpp-build-tools\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid decoding problems\n",
    "df = pd.read_csv(\"train.csv\")\n",
    " \n",
    "# encode questions to unicode\n",
    "# https://stackoverflow.com/a/6812069\n",
    "# ----------------- python 2 ---------------------\n",
    "# df['question1'] = df['question1'].apply(lambda x: unicode(str(x),\"utf-8\"))\n",
    "# df['question2'] = df['question2'].apply(lambda x: unicode(str(x),\"utf-8\"))\n",
    "# ----------------- python 3 ---------------------\n",
    "df['question1'] = df['question1'].apply(lambda x: str(x))\n",
    "df['question2'] = df['question2'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Text Preprocessing\n",
    "- Preprocessing:\n",
    "    - Removing html tags \n",
    "    - Removing Punctuations\n",
    "    - Performing stemming\n",
    "    - Removing Stopwords\n",
    "    - Expanding contractions etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preproces(x):\n",
    "    x = str(x).lower()\n",
    "    x = x.replace('%',' percent').replace('₹',' rupee').replace('$',' dollar').replace('€',' euro')\\\n",
    "                                .replace(',000,000','m').replace('000','k').replace('′',\"'\").replace(\"’\",\"'\")\\\n",
    "                                .replace(\"won't\",\"will not\").replace(\"can't\",'can not').replace(\"shouldn't\",\"should not\")\\\n",
    "                                .replace(\"what's\",'\"what is\"').replace(\"that's\",'that is').replace(\"he's\",\"he is\")\\\n",
    "                                .replace(\"she's\",\"she is\").replace(\"it's\",\"it is\").replace(\"'ve\",\" have\").replace(\"'re\",\" are\")\\\n",
    "                                .replace(\"'ll\",\" will\").replace(\"i'm\",\"i am\").replace(\"n't\", \" not\")\n",
    "    x = re.sub(r'([0-9]+)000000',r'\\1m',x)\n",
    "    x = re.sub(r'([0-9]+)000',r'\\1k',x)\n",
    "    \n",
    "    return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    df['question1'] = df['question1'].fillna(\"\").apply(text_preproces)\n",
    "    df['question2'] = df['question2'].fillna(\"\").apply(text_preproces)\n",
    "    print(\"token features...\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token features...\n"
     ]
    }
   ],
   "source": [
    "df = extract_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "df.question1=df.question1.apply(remove_urls)\n",
    "df.question2=df.question2.apply(remove_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "df.question1=df.question1.apply(remove_html)\n",
    "df.question2=df.question2.apply(remove_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Tokenization of Data\n",
    "-findall\n",
    "\n",
    "findall() is probably the single most powerful function in the re module. Above we used re.search() to find the first match for a pattern. findall() finds *all* the matches and returns them as a list of strings, with each string representing one match.\n",
    "\n",
    "#### Suppose we have a text with many email addresses\n",
    "  str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'\n",
    "\n",
    "#### Here re.findall() returns a list of all the found email strings\n",
    "  emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', str) ## ['alice@google.com', 'bob@abc.com']\n",
    "  for email in emails:\n",
    "    # do something with each found email string\n",
    "    print email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(txt):\n",
    "    tokens = re.findall(\"[\\w']+\", txt)\n",
    "    return tokens\n",
    "df.question1=df.question1.apply(word_tokenize)\n",
    "df.question2=df.question2.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatzation(lst):\n",
    "    new_lst=[]\n",
    "    for i in lst:\n",
    "        i=lemmatizer.lemmatize(i)\n",
    "        new_lst.append(i)\n",
    "    return new_lst\n",
    "df.question1=df.question1.apply(lemmatzation)\n",
    "df.question2=df.question2.apply(lemmatzation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how',\n",
       " 'can',\n",
       " 'internet',\n",
       " 'speed',\n",
       " 'be',\n",
       " 'increased',\n",
       " 'by',\n",
       " 'hacking',\n",
       " 'through',\n",
       " 'dns']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.question2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how',\n",
       " 'can',\n",
       " 'i',\n",
       " 'increase',\n",
       " 'the',\n",
       " 'speed',\n",
       " 'of',\n",
       " 'my',\n",
       " 'internet',\n",
       " 'connection',\n",
       " 'while',\n",
       " 'using',\n",
       " 'a',\n",
       " 'vpn']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.question1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_text_preprocessing.csv',index = 'False')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
